Model(
  (model): LlamaModel(
    (embed_tokens): Embedding(152064, 3584)
    (layers.0): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.1): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.2): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.3): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.4): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.5): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.6): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.7): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.8): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.9): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.10): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.11): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.12): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.13): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.14): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.15): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.16): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.17): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.18): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.19): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.20): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.21): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.22): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.23): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.24): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.25): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.26): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (layers.27): TransformerBlock(
      (self_attn): Attention(
        (q_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (k_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (v_proj): Linear(input_dims=3584, output_dims=512, bias=False)
        (o_proj): Linear(input_dims=3584, output_dims=3584, bias=False)
        (rope): RoPE(128, traditional=False)
      )
      (mlp): MLP(
        (gate_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
        (down_proj): Linear(input_dims=18944, output_dims=3584, bias=False)
        (up_proj): Linear(input_dims=3584, output_dims=18944, bias=False)
      )
      (input_layernorm): RMSNorm(3584, eps=1e-06)
      (post_attention_layernorm): RMSNorm(3584, eps=1e-06)
    )
    (norm): RMSNorm(3584, eps=1e-06)
  )
  (lm_head): Linear(input_dims=3584, output_dims=152064, bias=False)
)